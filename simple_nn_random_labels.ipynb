{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "p0Ljc86Nd5VB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.load_data import load_faces, load_features\n",
    "\n",
    "features = load_features()\n",
    "# faces = load_faces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1634870799262,
     "user": {
      "displayName": "Rohan Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjYtk9RJ4tvQkmTzl-z0jbeJ3r9CGjQZA8F6jwKOw=s64",
      "userId": "10846432523539896119"
     },
     "user_tz": 240
    },
    "id": "8elrw35hepty",
    "outputId": "4b6bbb92-9f7b-4e2c-933f-7f66f19cc257"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "# Check if GPU is available\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "W041psec26km"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1634876151666,
     "user": {
      "displayName": "Rohan Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjYtk9RJ4tvQkmTzl-z0jbeJ3r9CGjQZA8F6jwKOw=s64",
      "userId": "10846432523539896119"
     },
     "user_tz": 240
    },
    "id": "38RuMTtifnmm",
    "outputId": "c128a4ea-462a-4a80-b025-21ecec9c85b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1085795, 2048)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TNA_dHNszpfw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ground_truth = ['pol_dat_us', 'pol_dat_ca', 'pol_dat_uk', 'pol_fb_us']\n",
    "\n",
    "# from utils import label_func, get_labels\n",
    "\n",
    "# get_labels(faces.iloc[[1,2,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HV7cgmwPrNvz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Randomly split training and testing datasets\n",
    "np.random.seed(67)\n",
    "torch.manual_seed(67)\n",
    "\n",
    "num_of_samples = features.shape[0]\n",
    "idx = np.random.permutation(range(num_of_samples))\n",
    "cut = int(0.8 * num_of_samples)\n",
    "train_idx = idx[:cut]\n",
    "valid_idx = idx[cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(868636, 217159)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_idx), len(valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.46469116,  0.7696707 ,  0.28533868, ...,  0.87747054,\n",
       "        2.11865328, -0.50703953])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_labels = np.random.randint(0,2,size=num_of_samples).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1085795,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0rkOL3DxsANC"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, indexes):\n",
    "        self.indexes = indexes\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        index = self.indexes[i]\n",
    "        \n",
    "        sample = torch.tensor(features[index]).float()\n",
    "        label = random_labels[i]\n",
    "        \n",
    "        return sample,label\n",
    "    \n",
    "    def __len__(self): return len(self.indexes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = CustomDataset(train_idx)\n",
    "valid_ds = CustomDataset(valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=64, num_workers=6)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=64, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6111,  0.2389, -0.6342,  ...,  0.0221, -0.2860, -0.2405],\n",
       "        [ 0.7539,  1.9130, -0.1706,  ...,  0.3420,  0.2061, -0.5174],\n",
       "        [-0.6160,  0.4719, -0.5864,  ..., -0.5715,  3.1570, -0.5152],\n",
       "        ...,\n",
       "        [ 2.2167,  0.8044,  3.0293,  ..., -0.5455, -0.6586, -0.5183],\n",
       "        [ 2.4345,  1.0463, -0.6355,  ...,  3.4711,  1.0346, -0.1659],\n",
       "        [ 0.0053,  1.4662, -0.6334,  ..., -0.5711, -0.6581, -0.5154]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple binary classifier that takes a 2048 feature long tensor as input\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassifier, self).__init__()        \n",
    "        \n",
    "        # Number of input features is 2048\n",
    "        self.layer_1 = nn.Linear(2048, 2048)\n",
    "        self.layer_2 = nn.Linear(2048, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "model = BinaryClassifier()\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.2164],\n",
       "         [ 0.1106],\n",
       "         [ 0.0976],\n",
       "         [-0.1226],\n",
       "         [-0.0781]], grad_fn=<SliceBackward0>),\n",
       " torch.Size([64, 1]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run a minibatch of data through the model to check we get any errors\n",
    "preds = model(one_batch[0])\n",
    "preds[:5],preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    # Transform outputs to 0 and 1\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    # Calculate percentage of correct predictions\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4062)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_acc(preds, one_batch[1].unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify that we want to use the GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses_and_metrics(losses, val_losses, accuracies, val_accuracies):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 3))\n",
    "\n",
    "    axes[0].plot(losses)\n",
    "    axes[0].plot(val_losses)\n",
    "    axes[0].set_title('model loss')\n",
    "    axes[0].set_ylabel('loss')\n",
    "    axes[0].set_xlabel('epoch')\n",
    "    axes[0].legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "    axes[1].plot(accuracies)\n",
    "    axes[1].plot(val_accuracies)\n",
    "    axes[1].set_title('binary accuracy')\n",
    "    axes[1].set_ylabel('acc')\n",
    "    axes[1].set_xlabel('epoch')\n",
    "    axes[1].legend(['train', 'test'], loc='upper left')\n",
    "    \n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|████████████████████████| 13573/13573 [00:50<00:00, 267.37batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 0: | Training Loss: 0.69693 | Training accuracy: 0.5007356056140868 | Validation Loss: 2.78708555634986 | Validation Accuracy: 0.5017895287953199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████| 13573/13573 [00:51<00:00, 265.34batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 1: | Training Loss: 0.69478 | Training accuracy: 0.509069180151326 | Validation Loss: 2.77850067385845 | Validation Accuracy: 0.5021065272825497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████| 13573/13573 [00:51<00:00, 265.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 2: | Training Loss: 0.69295 | Training accuracy: 0.5220772134285462 | Validation Loss: 2.7712053648063253 | Validation Accuracy: 0.5018862067564277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|████████████████████████| 13573/13573 [00:50<00:00, 267.02batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 3: | Training Loss: 0.69046 | Training accuracy: 0.5345586629480548 | Validation Loss: 2.7612111838444595 | Validation Accuracy: 0.5007629009226033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|████████████████████████| 13573/13573 [00:50<00:00, 269.22batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 4: | Training Loss: 0.68755 | Training accuracy: 0.5448951305675935 | Validation Loss: 2.7495966587609257 | Validation Accuracy: 0.5009746716945538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|████████████████████████| 13573/13573 [00:50<00:00, 268.95batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 5: | Training Loss: 0.68432 | Training accuracy: 0.5544473139958489 | Validation Loss: 2.736674931528241 | Validation Accuracy: 0.5006524118241944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|████████████████████████| 13573/13573 [00:50<00:00, 271.21batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 6: | Training Loss: 0.68081 | Training accuracy: 0.5632869154643022 | Validation Loss: 2.7226368679510036 | Validation Accuracy: 0.5008135417593741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|████████████████████████| 13573/13573 [00:49<00:00, 271.57batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 7: | Training Loss: 0.67710 | Training accuracy: 0.5718078038357497 | Validation Loss: 2.707792886256983 | Validation Accuracy: 0.5006478081117607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|████████████████████████| 13573/13573 [00:50<00:00, 269.23batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 8: | Training Loss: 0.67303 | Training accuracy: 0.5798527604704676 | Validation Loss: 2.691530888674279 | Validation Accuracy: 0.5006247895495921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|████████████████████████| 13573/13573 [00:50<00:00, 271.25batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 9: | Training Loss: 0.66867 | Training accuracy: 0.5879848780711415 | Validation Loss: 2.6740820148332927 | Validation Accuracy: 0.5003202868582715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|███████████████████████| 13573/13573 [00:49<00:00, 271.67batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 10: | Training Loss: 0.66430 | Training accuracy: 0.5958440009621753 | Validation Loss: 2.656597651893692 | Validation Accuracy: 0.5002236088971637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|███████████████████████| 13573/13573 [00:50<00:00, 269.66batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 11: | Training Loss: 0.65954 | Training accuracy: 0.6032727460542214 | Validation Loss: 2.6375643334503605 | Validation Accuracy: 0.5013067966780385\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 12\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Move model to GPU if possible\n",
    "model = model.to(device)\n",
    "# Tells PyTorch we are in training mode\n",
    "model.train()\n",
    "\n",
    "try:\n",
    "    for e in range(EPOCHS):\n",
    "\n",
    "        # Set loss and accuracy to zero at start of each epoch\n",
    "        epoch_training_loss = 0\n",
    "        epoch_training_accuracy = 0\n",
    "        epoch_valid_loss = 0\n",
    "        epoch_valid_accuracy = 0\n",
    "\n",
    "        with tqdm(train_dl, unit=\"batch\") as tepoch:\n",
    "            for x_batch, y_batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {e}\")\n",
    "                # Transfer the tensors to the GPU if possible\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "                # Zero out gradients before backpropagation (PyTorch cumulates the gradient otherwise)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Predict a minibatch of outputs\n",
    "                y_pred = model(x_batch)\n",
    "\n",
    "                # Calculate the loss (unsqueeze adds a dimension to y)\n",
    "                loss = loss_function(y_pred, y_batch.unsqueeze(1))\n",
    "                training_acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "\n",
    "                # Backpropagation. Gradients are calculated\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_loss = loss.item()\n",
    "                batch_acc = training_acc.item()\n",
    "                epoch_training_loss += batch_loss\n",
    "                epoch_training_accuracy += batch_acc\n",
    "                losses.append(batch_loss)\n",
    "                accuracies.append(batch_acc)\n",
    "                \n",
    "                # tepoch.set_postfix(loss=loss.item(), accuracy=100. * training_acc.item())\n",
    "\n",
    "        for x_batch, y_batch in valid_dl:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            valid_y_pred = model(x_batch)\n",
    "            valid_loss = loss_function(valid_y_pred, y_batch.unsqueeze(1))\n",
    "            valid_acc = binary_acc(valid_y_pred, y_batch.unsqueeze(1))\n",
    "\n",
    "            batch_valid_loss = valid_loss.item()\n",
    "            batch_valid_accuracy = valid_acc.item()\n",
    "            epoch_valid_loss += batch_valid_loss\n",
    "            epoch_valid_accuracy += batch_valid_accuracy\n",
    "            val_losses.append(batch_valid_loss)\n",
    "            val_accuracies.append(batch_valid_accuracy)\n",
    "\n",
    "        avg_train_loss = epoch_training_loss/len(train_dl)\n",
    "        avg_valid_loss = epoch_training_loss/len(valid_dl)\n",
    "\n",
    "        avg_train_accuracy = epoch_training_accuracy/len(train_dl)\n",
    "        avg_valid_accuracy = epoch_valid_accuracy/len(valid_dl)\n",
    "\n",
    "        print(f'End of Epoch {e}: | Training Loss: {avg_train_loss:.5f} | Training accuracy: {avg_train_accuracy} | Validation Loss: {avg_valid_loss} | Validation Accuracy: {avg_valid_accuracy}')\n",
    "  \n",
    "except Exception as e:\n",
    "    print(\"Something went wrong in training\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "XDIEghli_AdB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217159"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_list = np.array([])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in valid_dl:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_tag = y_pred_tag.squeeze(1).cpu().numpy()\n",
    "        y_pred_list = np.append(y_pred_list, y_pred_tag)\n",
    "                     \n",
    "len(y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './saved_model/weights-2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO0fLK1v0FHPC7K2mZ78993",
   "mount_file_id": "1Gv9P6j20Iz6psk8kmk82m4fBHJmy2O-r",
   "name": "tabular_nn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
