{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5859,
     "status": "ok",
     "timestamp": 1634870670758,
     "user": {
      "displayName": "Rohan Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjYtk9RJ4tvQkmTzl-z0jbeJ3r9CGjQZA8F6jwKOw=s64",
      "userId": "10846432523539896119"
     },
     "user_tz": 240
    },
    "id": "SErhkr2Ib8xw",
    "outputId": "a3ada8be-6b33-4fd2-9e0c-57c5f9e9ec98"
   },
   "outputs": [],
   "source": [
    "!pip install -Uqq fastai fastbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "p0Ljc86Nd5VB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1634870799262,
     "user": {
      "displayName": "Rohan Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjYtk9RJ4tvQkmTzl-z0jbeJ3r9CGjQZA8F6jwKOw=s64",
      "userId": "10846432523539896119"
     },
     "user_tz": 240
    },
    "id": "8elrw35hepty",
    "outputId": "4b6bbb92-9f7b-4e2c-933f-7f66f19cc257"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "# Check if GPU is available\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "W041psec26km"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from fastai.data.core import DataLoaders, CrossEntropyLossFlat\n",
    "from fastai.data.block import CategoryBlock\n",
    "from fastai.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "K9_u1YabfLRs"
   },
   "outputs": [],
   "source": [
    "# Load a small chunk of the full dataset for testing purposes\n",
    "faces = pd.read_csv('./faces_smaller_chunk.csv')\n",
    "features = pd.read_csv('./vgg_chunks/chunk_1.csv', usecols=range(1, 2049))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1634876140507,
     "user": {
      "displayName": "Rohan Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjYtk9RJ4tvQkmTzl-z0jbeJ3r9CGjQZA8F6jwKOw=s64",
      "userId": "10846432523539896119"
     },
     "user_tz": 240
    },
    "id": "0Y_TyhY0fd4j",
    "outputId": "ff9a10a6-7c26-40d2-9b2c-779a091db7e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10859, 43)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1634876151666,
     "user": {
      "displayName": "Rohan Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjYtk9RJ4tvQkmTzl-z0jbeJ3r9CGjQZA8F6jwKOw=s64",
      "userId": "10846432523539896119"
     },
     "user_tz": 240
    },
    "id": "38RuMTtifnmm",
    "outputId": "c128a4ea-462a-4a80-b025-21ecec9c85b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10859, 2048)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "mSDaR2uSkPCS"
   },
   "outputs": [],
   "source": [
    "features_tensor = torch.tensor(features.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1634876151668,
     "user": {
      "displayName": "Rohan Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjYtk9RJ4tvQkmTzl-z0jbeJ3r9CGjQZA8F6jwKOw=s64",
      "userId": "10846432523539896119"
     },
     "user_tz": 240
    },
    "id": "xICurphSkgYE",
    "outputId": "4217bfac-d1b4-47e7-b2bf-83dfc2919f3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10859, 2048])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "MgJFtWngnAHL"
   },
   "outputs": [],
   "source": [
    "ground_truth = ['pol_dat_us', 'pol_dat_ca', 'pol_dat_uk', 'pol_fb_us']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "TNA_dHNszpfw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def label_func(row):\n",
    "    '''\n",
    "    Checks each column of ground_truth and extracts whichever column is not null as the label\n",
    "    '''\n",
    "    for i in ground_truth:\n",
    "        if ~np.isnan(row[i]):\n",
    "            return row[i]\n",
    "    return np.nan\n",
    "\n",
    "def get_labels(data):\n",
    "    '''\n",
    "    Returns array of labels from entire dataset\n",
    "    '''\n",
    "    return data.apply(lambda row: label_func(row), axis=1).to_numpy()\n",
    "\n",
    "get_labels(faces.iloc[[1,2,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "HV7cgmwPrNvz"
   },
   "outputs": [],
   "source": [
    "# Randomly split training and testing datasets\n",
    "num_of_samples = features_tensor.shape[0]\n",
    "idx = np.random.permutation(range(num_of_samples))\n",
    "\n",
    "cut = int(0.8 * num_of_samples)\n",
    "train_idx = idx[:cut]\n",
    "train_x = features_tensor[train_idx]\n",
    "train_y = get_labels(faces.iloc[train_idx])\n",
    "\n",
    "valid_idx = idx[cut:]\n",
    "valid_x = features_tensor[valid_idx]\n",
    "valid_y = get_labels(faces.iloc[train_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "0rkOL3DxsANC"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data,self.labels = data,labels\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        sample = self.data[i]\n",
    "        label = self.labels[i]\n",
    "        return sample,label\n",
    "    \n",
    "    def __len__(self): return len(self.data)\n",
    "\n",
    "train_ds = CustomDataset(train_x, train_y)\n",
    "valid_ds = CustomDataset(valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "6_HrPqMq3Bjb"
   },
   "outputs": [],
   "source": [
    "# Dataloader will be passing a batch of 64 samples at a time to our model\n",
    "train_dl = DataLoader(train_ds, batch_size=64)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple binary classifier that takes a 2048 feature long tensor as input\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassifier, self).__init__()        \n",
    "        \n",
    "        # Number of input features is 2048\n",
    "        self.layer_1 = nn.Linear(2048, 64) \n",
    "        self.layer_2 = nn.Linear(64, 64)\n",
    "        self.layer_out = nn.Linear(64, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = BinaryClassifier()\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5161],\n",
       "        [ 0.5439],\n",
       "        [-0.4218],\n",
       "        ...,\n",
       "        [ 0.0981],\n",
       "        [ 0.3573],\n",
       "        [-0.1066]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run a minibatch of data through the model to check we get any errors\n",
    "minibatch = train_x[:64]\n",
    "model.float()\n",
    "model(train_x.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    # Transform outputs to 0 and 1\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    # Calculate percentage of correct predictions\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify that we want to use the GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-39d548c76131>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mepoch_training_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtraining_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch_loss' is not defined"
     ]
    }
   ],
   "source": [
    "# Move model to GPU if possible\n",
    "model = model.to(device)\n",
    "# Tells PyTorch we are in training mode\n",
    "model.train()\n",
    "\n",
    "for e in range(0, EPOCHS):\n",
    "    \n",
    "    # Set loss and accuracy to zero at start of each epoch\n",
    "    epoch_training_loss = 0\n",
    "    epoch_training_accuracy = 0\n",
    "    epoch_valid_loss = 0\n",
    "    epoch_valid_accuracy = 0\n",
    "    \n",
    "    for x_batch, y_batch in train_dl:\n",
    "        # Transfer the tensors to the GPU if possible\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # Zero out gradients before backpropagation (PyTorch cumulates the gradient otherwise)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Predict a minibatch of outputs\n",
    "        y_pred = model(x_batch)\n",
    "        \n",
    "        # Calculate the loss (unsqueeze adds a dimension to y)\n",
    "        loss = loss_function(y_pred, y_batch.unsqueeze(1))\n",
    "        training_acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        # Backpropagation. Gradients are calculated\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_training_accuracy += training_acc.item()\n",
    "    \n",
    "    for x_batch, y_batch in valid_dl:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        valid_y_pred = model(x_batch)\n",
    "        valid_loss = loss_function(valid_y_pred, y_batch.unsqueeze(1))\n",
    "        valid_acc = binary_acc(valid_y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        epoch_valid_loss += valid_loss.item()\n",
    "        epoch_valid_accuracy += valid_acc.item()\n",
    "        \n",
    "    avg_train_loss = epoch_training_loss/len(train_dl)\n",
    "    avg_valid_loss = epoch_training_loss/len(valid_dl)\n",
    "    \n",
    "    avg_train_accuracy = epoch_training_accuracy/len(train_dl)\n",
    "    avg_valid_accuracy = epoch_valid_accuracy/len(valid_dl)\n",
    "    \n",
    "    print(f'Epoch {e}: | Training Loss: {avg_train_loss:.5f} | Training accuracy: {avg_train_accuracy} | Validation Loss: {avg_valid_loss} | Validation Accuracy: {avg_valid_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "XDIEghli_AdB"
   },
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "\n",
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in valid_dl:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = net(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqq sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [8687, 34]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-0fb1881f4b78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \"\"\"\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [8687, 34]"
     ]
    }
   ],
   "source": [
    "confusion_matrix(valid_labels, y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO0fLK1v0FHPC7K2mZ78993",
   "mount_file_id": "1Gv9P6j20Iz6psk8kmk82m4fBHJmy2O-r",
   "name": "tabular_nn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
