{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "manufactured-potential",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "# from loky import ProcessPoolExecutor  # for Windows users\n",
    "\n",
    "def parallel(func, iterable):\n",
    "    e = ProcessPoolExecutor()\n",
    "    return e.map(func, iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "furnished-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "twenty-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file_extensions = ('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')\n",
    "\n",
    "def is_image_path_valid(path: Path):\n",
    "    return path.is_file() and path.suffix in image_file_extensions\n",
    "\n",
    "def verify_image(fn):\n",
    "    \"Confirm that `fn` can be opened\"\n",
    "    try:\n",
    "        im = Image.open(fn)\n",
    "        im.draft(im.mode, (32,32))\n",
    "        im.load()\n",
    "        return True\n",
    "    except: return False\n",
    "\n",
    "def load_image_file(path):\n",
    "    return Image.open(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "mobile-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_recursively(root_dir: Path):\n",
    "    ls = os.listdir\n",
    "    \n",
    "    images = []\n",
    "    label2image = []\n",
    "    \n",
    "    def append_if_image(root: Path, filename: str):\n",
    "        path = root / filename\n",
    "        \n",
    "        if is_image_path_valid(path):\n",
    "            images.append(path)\n",
    "            label2image.append(root.stem)\n",
    "        \n",
    "    for filename in ls(root_dir):\n",
    "        file_path = root_dir / filename\n",
    "            \n",
    "        if file_path.is_dir():\n",
    "            for nested_filename in ls(file_path):\n",
    "                append_if_image(file_path, nested_filename)\n",
    "        else:\n",
    "            append_if_image(root_dir, filename)\n",
    "            \n",
    "    return images, label2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "nuclear-interpretation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "directed-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_paths = [Path(g) for g in glob(\"./data/new_image_crops/*\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "wooden-allergy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "595"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "loved-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "operating-robin",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "mean_rgb = (131.0912, 103.8827, 91.4953)\n",
    "\n",
    "def load_image_for_feature_extraction(path='', shape=None):\n",
    "    '''\n",
    "    Referenced from VGGFace2 Paper:\n",
    "    Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman, “VGGFace2: A dataset for recognising faces across pose and age,” arXiv:1710.08092 [cs], May 2018\n",
    "    '''\n",
    "    short_size = 224.0\n",
    "    crop_size = shape\n",
    "    img = Image.open(path)\n",
    "    im_shape = np.array(img.size)    # in the format of (width, height, *)\n",
    "    img = img.convert('RGB')\n",
    "\n",
    "    ratio = float(short_size) / np.min(im_shape)\n",
    "    img = img.resize(size=(int(np.ceil(im_shape[0] * ratio)),   # width\n",
    "                           int(np.ceil(im_shape[1] * ratio))),  # height\n",
    "                     resample=Image.BILINEAR)\n",
    "\n",
    "    x = np.array(img)  # image has been transposed into (height, width)\n",
    "    newshape = x.shape[:2]\n",
    "    h_start = (newshape[0] - crop_size[0])//2\n",
    "    w_start = (newshape[1] - crop_size[1])//2\n",
    "    x = x[h_start:h_start+crop_size[0], w_start:w_start+crop_size[1]]\n",
    "    \n",
    "    # normalize colors to prevent overfitting on color differences \n",
    "    x = x - mean_rgb\n",
    "    \n",
    "    # returns transformed image, and original image\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "greatest-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "image_size = (224,224,3)\n",
    "\n",
    "np.random.seed(67)\n",
    "\n",
    "def generate_batch(batch_size=16, shuffle=True):\n",
    "    total_samples = len(sample_paths)\n",
    "    \n",
    "    if shuffle:\n",
    "        idx = np.random.permutation(total_samples)\n",
    "    else:\n",
    "        idx = np.arange(total_samples)\n",
    "        \n",
    "    \n",
    "    for ndx in range(0, total_samples, batch_size):\n",
    "        batch_start = ndx\n",
    "        batch_end = np.min([ndx + batch_size, total_samples])\n",
    "        batch_idx = idx[batch_start: batch_end]\n",
    "        \n",
    "        batch_paths = np.array(sample_paths)[batch_idx]\n",
    "        \n",
    "        batch_images = []\n",
    "        batch_image2idx = []\n",
    "               \n",
    "        for i, (nid, path) in enumerate(zip(batch_idx, batch_paths)):\n",
    "            sub_image_paths = os.listdir(path)\n",
    "            \n",
    "            if(len(sub_image_paths) != 2):\n",
    "                warnings.warn(f\"{path} has {len(sub_image_paths)} files\")\n",
    "            else:\n",
    "                \n",
    "                batch_images.append(load_image_for_feature_extraction(path / sub_image_paths[0], image_size))\n",
    "                batch_images.append(load_image_for_feature_extraction(path / sub_image_paths[1], image_size))\n",
    "                batch_image2idx.append(nid)\n",
    "                batch_image2idx.append(nid)\n",
    "        \n",
    "        yield np.stack(batch_images), np.stack(batch_image2idx), (batch_start, batch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "tribal-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "from saved_model.prepare_resnet50 import prepare_resnet_model\n",
    "\n",
    "resnet_model = prepare_resnet_model(\"./saved_model/resnet50_ft_weight.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "perfect-jersey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 595\n"
     ]
    }
   ],
   "source": [
    "# Randomly split training and testing datasets\n",
    "np.random.seed(67)\n",
    "\n",
    "num_of_samples = len(sample_paths)\n",
    "print(f\"Total number of samples: {num_of_samples}\")\n",
    "\n",
    "idx = np.random.permutation(range(num_of_samples))\n",
    "cut = int(0.8 * num_of_samples)\n",
    "train_idx = idx[:cut]\n",
    "valid_idx = idx[cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "short-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "from saved_model.prepare_resnet50 import prepare_resnet_model\n",
    "\n",
    "resnet_model = prepare_resnet_model(\"./saved_model/resnet50_ft_weight.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bearing-march",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def extract_features(x):\n",
    "    x = torch.Tensor(x.transpose(0, 3, 1, 2))  # nx3x224x224\n",
    "    x = x.to(device)\n",
    "    x = resnet_model(x).detach().cpu().numpy()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "organizational-emergency",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_516581/725750957.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_image2idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "batch_images, batch_image2idx = next(iter(generate_batch()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = extract_features(batch_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "residential-premiere",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1190, 2048)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "elder-disposition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([186, 186, 241, 241, 478, 478])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_image2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "usual-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "continued-soccer",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 38/38 [00:04<00:00,  7.94it/s]\n"
     ]
    }
   ],
   "source": [
    "num_of_features = 2048\n",
    "features = np.zeros((num_of_samples * 2, num_of_features))\n",
    "features2idx = np.empty(num_of_samples * 2)\n",
    "labels = np.empty(num_of_samples * 2)\n",
    "batch_size = 16\n",
    "\n",
    "num_of_batches = math.ceil(num_of_samples / batch_size)\n",
    "\n",
    "for batch_images, batch_image2idx, batch_num in tqdm(generate_batch(batch_size=16, shuffle=True), total=num_of_batches):\n",
    "    batch_features = extract_features(batch_images)\n",
    "    \n",
    "    batch_start, batch_end = batch_num\n",
    "    batch_start = batch_start * 2\n",
    "    batch_end = np.min([batch_end * 2, num_of_samples * 2]) # because we are effectively doubling the batch_size \n",
    "    \n",
    "    features[batch_start:batch_end, :] = batch_features\n",
    "    features2idx[batch_start:batch_end] = batch_image2idx\n",
    "    \n",
    "    labels[batch_start:batch_end] = np.resize(np.arange(2), batch_end - batch_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "obvious-object",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1190, 2048)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "genuine-hungarian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.39536960e-02, 1.24970183e-01, 7.49640465e-02, ...,\n",
       "        3.43718678e-01, 0.00000000e+00, 1.10962498e+00],\n",
       "       [0.00000000e+00, 2.76269726e-02, 2.41232300e+00, ...,\n",
       "        0.00000000e+00, 2.37279248e+00, 2.05646253e+00],\n",
       "       [2.53713071e-01, 3.84531766e-02, 1.43994594e+00, ...,\n",
       "        1.73197722e+00, 7.25936413e-01, 7.65277743e-02],\n",
       "       ...,\n",
       "       [5.05050468e+00, 9.18740177e+00, 2.82824683e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.59333497e-02],\n",
       "       [1.85876331e+01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 1.94361019e+00, 0.00000000e+00],\n",
       "       [5.48500729e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.59657001e+00, 3.46146274e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "southeast-miller",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Backup\n",
    "np.save(\"./data/new_features.npy\", features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "searching-philosophy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "model = SGDClassifier(alpha=0.0001, penalty='elasticnet',loss='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "alleged-carbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "elect-machinery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(loss='log', penalty='elasticnet')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "seven-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "qualified-bishop",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
       "       1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "       0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
       "       0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
       "       1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
       "       0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
       "       1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0.])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "lined-economy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(pred, actual):\n",
    "    num_correct = (pred == actual).sum()\n",
    "    return num_correct / len(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "still-yesterday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5210084033613446"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_accuracy(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-software",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
