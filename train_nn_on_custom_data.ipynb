{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "hydraulic-hanging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1adc089950>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Randomly split training and testing datasets\n",
    "np.random.seed(67)\n",
    "torch.manual_seed(67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "spare-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "specified-throw",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11554717696, 2969567232, 2794906112)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "\n",
    "t, r, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "urban-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "from glob import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "herbal-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "# from loky import ProcessPoolExecutor  # for Windows users\n",
    "\n",
    "def parallel(func, iterable):\n",
    "    e = ProcessPoolExecutor()\n",
    "    return e.map(func, iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "gorgeous-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def verify_image(fn):\n",
    "    \"Confirm that `fn` can be opened\"\n",
    "    try:\n",
    "        im = Image.open(fn)\n",
    "        im.draft(im.mode, (32,32))\n",
    "        im.load()\n",
    "        return True\n",
    "    except: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "literary-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_paths = [Path(g) for g in glob(\"./data/new_image_crops/*\")]\n",
    "input_paths = np.array([(Path(path) / \"0.jpg\", Path(path) / \"1.jpg\") for path in sample_paths]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "designed-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_samples = len(input_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "trained-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_valid = parallel(verify_image, input_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "middle-technician",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1190"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([valid for valid in is_valid]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ignored-meeting",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "mean_rgb = (131.0912, 103.8827, 91.4953)\n",
    "\n",
    "def load_image_for_feature_extraction(path='', shape=None):\n",
    "    '''\n",
    "    Referenced from VGGFace2 Paper:\n",
    "    Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman, “VGGFace2: A dataset for recognising faces across pose and age,” arXiv:1710.08092 [cs], May 2018\n",
    "    '''\n",
    "    short_size = 224.0\n",
    "    crop_size = shape\n",
    "    img = Image.open(path)\n",
    "    im_shape = np.array(img.size)    # in the format of (width, height, *)\n",
    "    img = img.convert('RGB')\n",
    "\n",
    "    ratio = float(short_size) / np.min(im_shape)\n",
    "    img = img.resize(size=(int(np.ceil(im_shape[0] * ratio)),   # width\n",
    "                           int(np.ceil(im_shape[1] * ratio))),  # height\n",
    "                     resample=Image.BILINEAR)\n",
    "\n",
    "    x = np.array(img)  # image has been transposed into (height, width)\n",
    "    newshape = x.shape[:2]\n",
    "    h_start = (newshape[0] - crop_size[0])//2\n",
    "    w_start = (newshape[1] - crop_size[1])//2\n",
    "    x = x[h_start:h_start+crop_size[0], w_start:w_start+crop_size[1]]\n",
    "    \n",
    "    # normalize colors to prevent overfitting on color differences \n",
    "    x = x - mean_rgb\n",
    "    \n",
    "    # returns transformed image, and original image\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "registered-straight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "image_size = (224,224,3)\n",
    "\n",
    "np.random.seed(67)\n",
    "\n",
    "def generate_batch(batch_size=16, shuffle=False):\n",
    "    total_samples = len(sample_paths)\n",
    "    \n",
    "    if shuffle:\n",
    "        idx = np.random.permutation(total_samples)\n",
    "    else:\n",
    "        idx = np.arange(total_samples)\n",
    "        \n",
    "    \n",
    "    for ndx in range(0, total_samples, batch_size):\n",
    "        batch_start = ndx\n",
    "        batch_end = np.min([ndx + batch_size, total_samples])\n",
    "        batch_idx = idx[batch_start: batch_end]\n",
    "        \n",
    "        batch_paths = np.array(sample_paths)[batch_idx]\n",
    "        \n",
    "        batch_images = []\n",
    "        batch_image2idx = []\n",
    "               \n",
    "        for i, (nid, path) in enumerate(zip(batch_idx, batch_paths)):\n",
    "            sub_image_paths = os.listdir(path)\n",
    "            \n",
    "            if(len(sub_image_paths) != 2):\n",
    "                warnings.warn(f\"{path} has {len(sub_image_paths)} files\")\n",
    "            else:\n",
    "                \n",
    "                batch_images.append(load_image_for_feature_extraction(path / sub_image_paths[0], image_size))\n",
    "                batch_images.append(load_image_for_feature_extraction(path / sub_image_paths[1], image_size))\n",
    "                batch_image2idx.append(nid)\n",
    "                batch_image2idx.append(nid)\n",
    "        \n",
    "        yield np.stack(batch_images), np.stack(batch_image2idx), (batch_start, batch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "scenic-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "from saved_model.prepare_resnet50 import prepare_resnet_model\n",
    "\n",
    "resnet_model = prepare_resnet_model(\"./saved_model/resnet50_ft_weight.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "accompanied-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_features = 2048\n",
    "\n",
    "features = torch.empty((len(input_paths), num_of_features)).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "false-texas",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 38/38 [00:05<00:00,  7.43it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "num_of_batches = math.ceil(len(sample_paths) / batch_size)\n",
    "\n",
    "for batch_images, batch_image2idx, batch_num in tqdm(generate_batch(batch_size=batch_size), total=num_of_batches):\n",
    "    batch_start = batch_num[0] * 2\n",
    "    batch_end = np.min([batch_num[1] * 2, len(input_paths)])\n",
    "    \n",
    "    x = torch.Tensor(batch_images.transpose(0, 3, 1, 2))  # nx3x224x224\n",
    "    x = x.to(device)\n",
    "    feat = resnet_model(x).cpu().detach()\n",
    "    \n",
    "    features[batch_start:batch_end, :] = feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "supposed-mongolia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.9841, 0.3229, 0.1657,  ..., 0.0000, 1.5662, 0.0187])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "faced-starter",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, indexes):\n",
    "        self.indexes = indexes\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.indexes)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        index = self.indexes[i]\n",
    "        return features[index], labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "velvet-cache",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 1190\n"
     ]
    }
   ],
   "source": [
    "# Randomly split training and testing datasets\n",
    "np.random.seed(67)\n",
    "\n",
    "num_of_samples = len(input_paths)\n",
    "print(f\"Total number of samples: {num_of_samples}\")\n",
    "\n",
    "idx = np.random.permutation(range(num_of_samples))\n",
    "cut = int(0.8 * num_of_samples)\n",
    "train_idx = idx[:cut]\n",
    "valid_idx = idx[cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "experienced-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "labels = np.resize(np.array([0, 1]), len(input_paths))\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_ds = CustomDataset(train_idx)\n",
    "valid_ds = CustomDataset(valid_idx)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "restricted-survivor",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_features, batch_labels = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dietary-relations",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2048])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "detected-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple binary classifier that takes a 2048 feature long tensor as input\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassifier, self).__init__()        \n",
    "        \n",
    "        # Number of input features is 2048\n",
    "        self.layer_1 = nn.Linear(2048, 2048)\n",
    "        self.layer_2 = nn.Linear(2048, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fallen-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "model = BinaryClassifier()\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ideal-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    # Transform outputs to 0 and 1\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    # Calculate percentage of correct predictions\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "crazy-edinburgh",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████████████████████████| 60/60 [00:00<00:00, 385.33batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 0: | Training Loss: 0.02948 | Training accuracy: 0.9989583333333333 | Validation Loss: 0.11792726715405782 | Validation Accuracy: 0.4607142865657806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████| 60/60 [00:00<00:00, 438.32batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 1: | Training Loss: 0.02501 | Training accuracy: 0.9979166666666667 | Validation Loss: 0.10005000693102678 | Validation Accuracy: 0.47321428656578063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████████████████████████| 60/60 [00:00<00:00, 440.19batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 2: | Training Loss: 0.01834 | Training accuracy: 1.0 | Validation Loss: 0.07336405261109273 | Validation Accuracy: 0.4648809532324473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████████████████████████| 60/60 [00:00<00:00, 411.09batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 3: | Training Loss: 0.01516 | Training accuracy: 1.0 | Validation Loss: 0.060644253715872766 | Validation Accuracy: 0.4648809532324473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████████████████████████| 60/60 [00:00<00:00, 448.05batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 4: | Training Loss: 0.01291 | Training accuracy: 1.0 | Validation Loss: 0.051637458894401786 | Validation Accuracy: 0.45654761989911397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████████████████████████| 60/60 [00:00<00:00, 424.60batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 5: | Training Loss: 0.01047 | Training accuracy: 1.0 | Validation Loss: 0.04187389245877663 | Validation Accuracy: 0.4648809532324473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████████████████████████| 60/60 [00:00<00:00, 443.31batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 6: | Training Loss: 0.00954 | Training accuracy: 1.0 | Validation Loss: 0.03815561728551984 | Validation Accuracy: 0.4523809532324473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████████████████████████| 60/60 [00:00<00:00, 441.99batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 7: | Training Loss: 0.00783 | Training accuracy: 1.0 | Validation Loss: 0.03132592967400948 | Validation Accuracy: 0.4648809532324473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████████████████████████| 60/60 [00:00<00:00, 443.44batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 8: | Training Loss: 0.00718 | Training accuracy: 1.0 | Validation Loss: 0.02870678637797634 | Validation Accuracy: 0.4648809532324473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████████████████████████| 60/60 [00:00<00:00, 441.62batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 9: | Training Loss: 0.00639 | Training accuracy: 1.0 | Validation Loss: 0.025556146011998255 | Validation Accuracy: 0.4648809532324473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|█████████████████████████████| 60/60 [00:00<00:00, 442.14batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 10: | Training Loss: 0.00570 | Training accuracy: 1.0 | Validation Loss: 0.022786633297801017 | Validation Accuracy: 0.46964285771052044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|█████████████████████████████| 60/60 [00:00<00:00, 446.42batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 11: | Training Loss: 0.00516 | Training accuracy: 1.0 | Validation Loss: 0.02064001321171721 | Validation Accuracy: 0.4607142865657806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|█████████████████████████████| 60/60 [00:00<00:00, 449.76batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 12: | Training Loss: 0.00464 | Training accuracy: 1.0 | Validation Loss: 0.018567743059247733 | Validation Accuracy: 0.48571428656578064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|█████████████████████████████| 60/60 [00:00<00:00, 448.22batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 13: | Training Loss: 0.00412 | Training accuracy: 1.0 | Validation Loss: 0.016484972182661296 | Validation Accuracy: 0.4648809532324473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|█████████████████████████████| 60/60 [00:00<00:00, 446.77batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 14: | Training Loss: 0.00364 | Training accuracy: 1.0 | Validation Loss: 0.014551196433603763 | Validation Accuracy: 0.469047619899114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|█████████████████████████████| 60/60 [00:00<00:00, 449.06batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 15: | Training Loss: 0.00333 | Training accuracy: 1.0 | Validation Loss: 0.013332185770074527 | Validation Accuracy: 0.47321428656578063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|█████████████████████████████| 60/60 [00:00<00:00, 448.16batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 16: | Training Loss: 0.00302 | Training accuracy: 1.0 | Validation Loss: 0.012072455851982038 | Validation Accuracy: 0.45714285771052043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|█████████████████████████████| 60/60 [00:00<00:00, 444.63batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 17: | Training Loss: 0.00278 | Training accuracy: 1.0 | Validation Loss: 0.011124632973223924 | Validation Accuracy: 0.469047619899114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|█████████████████████████████| 60/60 [00:00<00:00, 444.86batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 18: | Training Loss: 0.00255 | Training accuracy: 1.0 | Validation Loss: 0.010217606912677486 | Validation Accuracy: 0.47321428656578063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|█████████████████████████████| 60/60 [00:00<00:00, 448.61batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 19: | Training Loss: 0.00243 | Training accuracy: 1.0 | Validation Loss: 0.009735634985069434 | Validation Accuracy: 0.4821428577105204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|█████████████████████████████| 60/60 [00:00<00:00, 448.39batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 20: | Training Loss: 0.00221 | Training accuracy: 1.0 | Validation Loss: 0.00883532288329055 | Validation Accuracy: 0.45654761989911397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|█████████████████████████████| 60/60 [00:00<00:00, 448.14batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 21: | Training Loss: 0.00199 | Training accuracy: 1.0 | Validation Loss: 0.007943761100371679 | Validation Accuracy: 0.4773809532324473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|█████████████████████████████| 60/60 [00:00<00:00, 443.71batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 22: | Training Loss: 0.00189 | Training accuracy: 1.0 | Validation Loss: 0.007565606959785024 | Validation Accuracy: 0.47797619104385375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|█████████████████████████████| 60/60 [00:00<00:00, 440.89batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 23: | Training Loss: 0.00176 | Training accuracy: 1.0 | Validation Loss: 0.007044565522422394 | Validation Accuracy: 0.469047619899114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|█████████████████████████████| 60/60 [00:00<00:00, 444.96batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 24: | Training Loss: 0.00163 | Training accuracy: 1.0 | Validation Loss: 0.006538837992896636 | Validation Accuracy: 0.469047619899114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|█████████████████████████████| 60/60 [00:00<00:00, 444.21batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 25: | Training Loss: 0.00151 | Training accuracy: 1.0 | Validation Loss: 0.006049358802071462 | Validation Accuracy: 0.4863095243771871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|█████████████████████████████| 60/60 [00:00<00:00, 444.71batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 26: | Training Loss: 0.00140 | Training accuracy: 1.0 | Validation Loss: 0.005593762034550309 | Validation Accuracy: 0.46547619104385374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|█████████████████████████████| 60/60 [00:00<00:00, 444.32batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 27: | Training Loss: 0.00133 | Training accuracy: 1.0 | Validation Loss: 0.005329897149931639 | Validation Accuracy: 0.4773809532324473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|█████████████████████████████| 60/60 [00:00<00:00, 443.47batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 28: | Training Loss: 0.00123 | Training accuracy: 1.0 | Validation Loss: 0.004904818313661963 | Validation Accuracy: 0.4773809532324473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|█████████████████████████████| 60/60 [00:00<00:00, 443.64batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 29: | Training Loss: 0.00114 | Training accuracy: 1.0 | Validation Loss: 0.004550012670612584 | Validation Accuracy: 0.4821428577105204\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Move model to GPU if possible\n",
    "model = model.to(device)\n",
    "# Tells PyTorch we are in training mode\n",
    "model.train()\n",
    "\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "\n",
    "    # Set loss and accuracy to zero at start of each epoch\n",
    "    epoch_training_loss = 0\n",
    "    epoch_training_accuracy = 0\n",
    "    epoch_valid_loss = 0\n",
    "    epoch_valid_accuracy = 0\n",
    "\n",
    "    with tqdm(train_dl, unit=\"batch\") as tepoch:\n",
    "        for x_batch, y_batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {e}\")\n",
    "            # Transfer the tensors to the GPU if possible\n",
    "            x_batch = x_batch.to(device, dtype=torch.float)\n",
    "            y_batch = y_batch.to(device, dtype=torch.float)\n",
    "\n",
    "            # Zero out gradients before backpropagation (PyTorch cumulates the gradient otherwise)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Predict a minibatch of outputs\n",
    "            y_pred = model(x_batch)\n",
    "\n",
    "            # Calculate the loss (unsqueeze adds a dimension to y)\n",
    "\n",
    "\n",
    "\n",
    "            loss = loss_function(y_pred, y_batch.unsqueeze(1))\n",
    "            training_acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "\n",
    "            # Backpropagation. Gradients are calculated\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss = loss.item()\n",
    "            batch_acc = training_acc.item()\n",
    "            epoch_training_loss += batch_loss\n",
    "            epoch_training_accuracy += batch_acc\n",
    "            losses.append(batch_loss)\n",
    "            accuracies.append(batch_acc)\n",
    "\n",
    "            # tepoch.set_postfix(loss=loss.item(), accuracy=100. * training_acc.item())\n",
    "\n",
    "    for x_batch, y_batch in valid_dl:\n",
    "        x_batch = x_batch.to(device, dtype=torch.float)\n",
    "        y_batch = y_batch.to(device, dtype=torch.float)\n",
    "\n",
    "        valid_y_pred = model(x_batch)\n",
    "        valid_loss = loss_function(valid_y_pred, y_batch.unsqueeze(1))\n",
    "        valid_acc = binary_acc(valid_y_pred, y_batch.unsqueeze(1))\n",
    "\n",
    "        batch_valid_loss = valid_loss.item()\n",
    "        batch_valid_accuracy = valid_acc.item()\n",
    "        epoch_valid_loss += batch_valid_loss\n",
    "        epoch_valid_accuracy += batch_valid_accuracy\n",
    "        val_losses.append(batch_valid_loss)\n",
    "        val_accuracies.append(batch_valid_accuracy)\n",
    "\n",
    "    avg_train_loss = epoch_training_loss/len(train_dl)\n",
    "    avg_valid_loss = epoch_training_loss/len(valid_dl)\n",
    "\n",
    "    avg_train_accuracy = epoch_training_accuracy/len(train_dl)\n",
    "    avg_valid_accuracy = epoch_valid_accuracy/len(valid_dl)\n",
    "\n",
    "    print(f'End of Epoch {e}: | Training Loss: {avg_train_loss:.5f} | Training accuracy: {avg_train_accuracy} | Validation Loss: {avg_valid_loss} | Validation Accuracy: {avg_valid_accuracy}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-instrument",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
